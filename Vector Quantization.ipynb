{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"Vector Quantization.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"kk7Rq5Q8nwdP"},"source":["<br><font size=\"5\">必要なライブラリのインポート</font></br>"]},{"cell_type":"code","metadata":{"id":"yuIhaep5nwdZ"},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import keras\n","import os\n","import h5py\n","from keras.utils import to_categorical\n","from keras.models import Sequential, load_model, model_from_json,model_from_yaml\n","from keras.layers import  Input,Dense, Dropout,Flatten, Conv2D, MaxPooling2D\n","from keras.layers import BatchNormalization,Activation\n","from keras.datasets import mnist,cifar10,cifar100\n","from sklearn.model_selection import train_test_split\n","from keras import regularizers\n","from keras import backend as K\n","\n","import matplotlib\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as mtick\n","\n","import scipy\n","from scipy.stats import norm\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p_Zfr1f-nwda"},"source":["<br><font size=\"5\">Keras公式のVGG16(ImageNet)をダウンロード</font><br>\n","<font size=\"5\" color=\"red\">※本実験では用いない</font>"]},{"cell_type":"code","metadata":{"id":"ZRZHQ7mnnwdb"},"source":["model=keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1oSoBUS0nwdb"},"source":["<br><font size=\"5\">データセット：MNISTの場合</font><br>\n","<font size=\"5\" color=\"red\">※本実験では用いない</font>"]},{"cell_type":"code","metadata":{"id":"KoV_DAQ7nwdb"},"source":["(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# 28 x 28の画像がgrayscaleで1chなので、28, 28, 1にreshapeする\n","x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n","x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n","\n","#0-255の整数値を0〜1の小数に変換する\n","#MNISTって必ずこの処理入るけれど、意味あるのかな\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","\n","# one-hot vector形式に変換する\n","y_train = keras.utils.to_categorical(y_train, 10)\n","y_test = keras.utils.to_categorical(y_test, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4q1e3Vonwdc"},"source":["<br><font size=\"5\">データセット：Cifar-10の場合</font>"]},{"cell_type":"code","metadata":{"id":"IC1uT_9Vnwdc","outputId":"3a11a532-e23d-4ec8-acea-68d3f98ab64c"},"source":["(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","\n","# one-hot vector形式に変換する\n","y_train = keras.utils.to_categorical(y_train, 10)\n","y_test = keras.utils.to_categorical(y_test, 10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x_train shape: (50000, 32, 32, 3)\n","50000 train samples\n","10000 test samples\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ty-PtXTMnwdd"},"source":["<br><font size=\"5\">データセット：Cifar-100の場合</font>"]},{"cell_type":"code","metadata":{"id":"-OkVhSFEnwde","outputId":"ea06d0ec-e16f-45f3-b3c7-ea2fd173bb2e"},"source":["(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n","\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","\n","# one-hot vector形式に変換する\n","y_train = keras.utils.to_categorical(y_train, 100)\n","y_test = keras.utils.to_categorical(y_test, 100)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x_train shape: (50000, 32, 32, 3)\n","50000 train samples\n","10000 test samples\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e-4EGeNUnwde"},"source":["<br><font size=\"5\">ネットワーク：VGG-16<br>\n","データセット：MNIST,Cifar-10,Cifar-100の場合<br>\n","重みの初期値：Xavier<br>\n","クラス分類</font>"]},{"cell_type":"code","metadata":{"id":"TuPhRhabnwdf"},"source":["model = Sequential()\n","\n","#MNISTの場合\n","#model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same', name=\"Conv1\",input_shape=(28,28,1)))\n","#cifar10,cifar100の場合\n","model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same', name=\"Conv1\",input_shape=(32, 32, 3)))\n","model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same', name=\"Conv2\"))\n","model.add(BatchNormalization(name=\"Norm1\"))\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same',name=\"Pool1\"))\n","\n","model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same',name=\"Conv3\"))\n","model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1),activation='relu', padding='same',name=\"Conv4\"))\n","model.add(BatchNormalization(name=\"Norm2\"))\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same',name=\"Pool2\"))\n","\n","model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same',name=\"Conv5\"))\n","model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same',name=\"Conv6\"))\n","model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same',name=\"Conv7\"))\n","model.add(BatchNormalization(name=\"Norm3\"))\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same',name=\"Pool3\"))\n","\n","model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same',name=\"Conv8\"))\n","model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same',name=\"Conv9\"))\n","model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same',name=\"Conv10\"))\n","model.add(BatchNormalization(name=\"Norm4\"))\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same',name=\"Pool4\"))\n","\n","model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same',name=\"Conv11\"))\n","model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same',name=\"Conv12\"))\n","model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same',name=\"Conv13\"))\n","model.add(BatchNormalization(name=\"Norm5\"))\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same',name=\"Pool5\"))\n","\n","model.add(Flatten(name=\"Flat\"))\n","\n","model.add(Dense(units=512,activation=\"relu\",name=\"Dense1\"))\n","model.add(BatchNormalization(name=\"Norm6\"))\n","\n","model.add(Dense(units=512,activation=\"relu\",name=\"Dense2\"))\n","model.add(BatchNormalization(name=\"Norm7\"))\n","\n","#MNIST,cifar10の場合\n","#model.add(Dense(units=10, activation='softmax',name='Dense3'))\n","#cifar100の場合\n","model.add(Dense(units=100, activation='softmax',name='Dense3'))\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OFS2Nn0Hnwdf"},"source":["<br><font size=\"5\">ネットワーク：VGG-16<br>\n","データセット：MNIST,Cifar-10,Cifar-100の場合<br>\n","重みの初期値：He_normal(+正則化L2)<br>\n","クラス分類</font>"]},{"cell_type":"code","metadata":{"id":"aFAbeIMcnwdf"},"source":["from keras.regularizers import l2\n","\n","model = Sequential()\n","\n","#MNISTの場合\n","#model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same', name=\"Conv1\",input_shape=(28,28,1)))\n","#cifar10,cifar100の場合\n","model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),padding='same', name=\"Conv1\",input_shape=(32, 32, 3)))\n","model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),padding='same', name=\"Conv2\"))\n","model.add(BatchNormalization(name=\"Norm1\"))\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same',name=\"Pool1\"))\n","\n","model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu',kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),padding='same',name=\"Conv3\"))\n","model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1),activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),padding='same',name=\"Conv4\"))\n","model.add(BatchNormalization(name=\"Norm2\"))\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same',name=\"Pool2\"))\n","\n","model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),padding='same',name=\"Conv5\"))\n","model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),padding='same',name=\"Conv6\"))\n","model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),padding='same',name=\"Conv7\"))\n","model.add(BatchNormalization(name=\"Norm3\"))\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same',name=\"Pool3\"))\n","\n","model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),padding='same',name=\"Conv8\"))\n","model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),padding='same',name=\"Conv9\"))\n","model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),padding='same',name=\"Conv10\"))\n","model.add(BatchNormalization(name=\"Norm4\"))\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same',name=\"Pool4\"))\n","\n","model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),padding='same',name=\"Conv11\"))\n","model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),padding='same',name=\"Conv12\"))\n","model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),padding='same',name=\"Conv13\"))\n","model.add(BatchNormalization(name=\"Norm5\"))\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same',name=\"Pool5\"))\n","\n","model.add(Flatten(name=\"Flat\"))\n","model.add(Dense(units=512,activation='relu',kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),name=\"Dense1\"))\n","model.add(BatchNormalization(name=\"Norm6\"))\n","#model.add(Activation('relu',name=\"Activation1\"))\n","\n","model.add(Dense(units=512,activation='relu',kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),name=\"Dense2\"))\n","model.add(BatchNormalization(name=\"Norm7\"))\n","#model.add(Activation('relu',name=\"Activation2\"))\n","\n","#MNIST,cifar10の場合\n","#model.add(Dense(units=10, activation='softmax', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),name='Dense3'))\n","#cifar100の場合\n","model.add(Dense(units=100, activation='softmax', kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(1e-4),name='Dense3'))\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c11MhYihnwdg"},"source":["<br><font size=\"5\">モデルコンパイル</font>"]},{"cell_type":"code","metadata":{"id":"Xfwi9tvCnwdh","outputId":"fd292e8a-44f7-4e7f-e448-163a33183754"},"source":["from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","\n","'''\n","ここで，学習率減衰したかったが，実装できなかった\n","'''\n","\n","# def lr_schedule(epoch):\n","#     lrate=0.001\n","#     if epoch>=100:\n","#         lrate=0.0002\n","#     elif epoch>=150:\n","#         lrate=0.00004\n","#     return lrate\n","\n","\n","# scheduler = LearningRateScheduler(schedule=lr_schedule)\n","\n","adam = Adam(lr=0.001) # 最適化手法(Adam)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=adam, \n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"EOL while scanning string literal (<ipython-input-3-44f9b1efee92>, line 8)","traceback":["\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-44f9b1efee92>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    '''''''''''''''''''\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"]}]},{"cell_type":"markdown","metadata":{"id":"TFkbFZg7nwdi"},"source":["<br><font size=\"5\">モデルの学習</font>"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"x0a_Xqjfnwdi"},"source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","#データオーグメンテーション\n","datagen = ImageDataGenerator(\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    channel_shift_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True\n","    #vertical_flip=True\n",")\n","\n","# 訓練用パラメータの設定.\n","batch_size = 128\n","epochs = 200\n","\n","history = model.fit_generator(\n","    datagen.flow(x_train, y_train, batch_size=batch_size),\n","    steps_per_epoch=x_train.shape[0] // batch_size, \n","    validation_data=(x_test, y_test),\n","    epochs=epochs, \n","    verbose=1,)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ar_q1qXZnwdi"},"source":["<br><font size=\"5\">モデルの精度と損失のグラフをプロット</font>"]},{"cell_type":"code","metadata":{"id":"38fv0NzAnwdj"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","def plot_history(history):\n","    # 精度の履歴をプロット\n","    plt.plot(history.history['accuracy'],\"-\",label=\"accuracy\")\n","    plt.plot(history.history['val_accuracy'],\"-\",label=\"val_acc\")\n","    plt.title('model accuracy')\n","    plt.xlabel('epoch')\n","    plt.ylabel('accuracy')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","    # 損失の履歴をプロット\n","    plt.plot(history.history['loss'],\"-\",label=\"loss\",)\n","    plt.plot(history.history['val_loss'],\"-\",label=\"val_loss\")\n","    plt.title('model loss')\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.legend(loc='upper right')\n","    plt.show()\n","# modelに学習させた時の変化の様子をplot\n","plot_history(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uo8OPhULnwdj"},"source":["<br><font size=\"5\">モデルの精度のグラフを保存</font>"]},{"cell_type":"code","metadata":{"id":"kxB_343Bnwdj"},"source":["#ファイル名を決める\n","f_model='./VGG16_cifar100_epoch200_he_normal'\n","\n","matplotlib.use('Agg') \n","plt.plot(history.history[\"val_accuracy\"],marker=\"x\",label=\"val_accuracy\")\n","plt.plot(history.history[\"accuracy\"],marker=\".\",label=\"accuracy\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.legend(loc=\"best\")\n","plt.savefig(os.path.join(f_model, \"認識率_accuracy\"+\".tif\"))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mG6o66qfnwdk"},"source":["<br><font size=\"5\">モデルの損失のグラフを保存</font>"]},{"cell_type":"code","metadata":{"id":"_skCCrm1nwdk"},"source":["plt.plot(history.history[\"val_loss\"],marker=\"x\",label=\"val_loss\") \n","plt.plot(history.history[\"loss\"],marker=\".\",label=\"loss\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"loss\")\n","plt.legend(loc=\"best\")\n","plt.savefig(os.path.join(f_model, \"認識率_loss\"+\".tif\"))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rzm-828wnwdk"},"source":["<br><font size=\"5\">モデルの重み係数を保存</font>"]},{"cell_type":"code","metadata":{"id":"z8F158YXnwdk","outputId":"bb985891-8635-4590-e368-36266b6c7009"},"source":["# f_model=\"VGG16_\"   #\"\"(ダブルコーテーション)又は''(シングルコーテーション)\n","# step1=\"cifar100_epoch200_vector2_class256\"      #\"\"(ダブルコーテーション)又は''(シングルコーテーション)   \n","\n","#f_model2=\"VGG16_cifar100_epoch200_vector2_class128\"\n","#f_model2=\"./VGG16_he_normal/cifar10/vector2_1_8_400_9_13_10(1)\"\n","f_model2=\"./VGG16_he_normal/cifar100/vector3/class10(1)\"\n","#f_model2=\"./VGG16/cifar100/vector4/class100(1)\"\n","#f_model2=f_model+\"_step\"+str(step1)\n","\n","os.mkdir(f_model2)\n","print('save the architecture of a model')\n","json_string=model.to_json()                                           #モデル構造の保存(9行目まで)\n","open(os.path.join(f_model2,'cnn_model.json'),'w').write(json_string)\n","yaml_string=model.to_yaml()\n","open(os.path.join(f_model2,'cnn_model.yaml'),'w').write(yaml_string)\n","print('save weights')\n","model.save_weights(os.path.join(f_model2,'cnn_model_weights.hdf5'))  #重み係数の保存\n","print('save all files')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["save the architecture of a model\n","save weights\n","save all files\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"30MjzXzpnwdl"},"source":["<br><font size=\"5\">学習済みモデルの読み込み</font>"]},{"cell_type":"code","metadata":{"id":"G3UqeUwSnwdl","outputId":"470ceba7-6f00-4f2f-da01-9c7740b5c442"},"source":["f_model = './VGG16_cifar10_epoch200_he_normal'\n","#f_model = './VGG16_he_normal/cifar10/vector2/class15(1)'\n","#f_model='./処理後/VGG16_cifar10_epoch200_3_(1)512_(2_11)256_(12_13)16'\n","model_filename = 'cnn_model.json'\n","weights_filename = 'cnn_model_weights.hdf5'\n","\n","json_string = open(os.path.join(f_model, model_filename)).read()\n","model = model_from_json(json_string)\n","model.load_weights(os.path.join(f_model,weights_filename)) #学習済みの重みを利用\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","Conv1 (Conv2D)               (None, 32, 32, 64)        1792      \n","_________________________________________________________________\n","Conv2 (Conv2D)               (None, 32, 32, 64)        36928     \n","_________________________________________________________________\n","Norm1 (BatchNormalization)   (None, 32, 32, 64)        256       \n","_________________________________________________________________\n","Pool1 (MaxPooling2D)         (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","Conv3 (Conv2D)               (None, 16, 16, 128)       73856     \n","_________________________________________________________________\n","Conv4 (Conv2D)               (None, 16, 16, 128)       147584    \n","_________________________________________________________________\n","Norm2 (BatchNormalization)   (None, 16, 16, 128)       512       \n","_________________________________________________________________\n","Pool2 (MaxPooling2D)         (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","Conv5 (Conv2D)               (None, 8, 8, 256)         295168    \n","_________________________________________________________________\n","Conv6 (Conv2D)               (None, 8, 8, 256)         590080    \n","_________________________________________________________________\n","Conv7 (Conv2D)               (None, 8, 8, 256)         590080    \n","_________________________________________________________________\n","Norm3 (BatchNormalization)   (None, 8, 8, 256)         1024      \n","_________________________________________________________________\n","Pool3 (MaxPooling2D)         (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","Conv8 (Conv2D)               (None, 4, 4, 512)         1180160   \n","_________________________________________________________________\n","Conv9 (Conv2D)               (None, 4, 4, 512)         2359808   \n","_________________________________________________________________\n","Conv10 (Conv2D)              (None, 4, 4, 512)         2359808   \n","_________________________________________________________________\n","Norm4 (BatchNormalization)   (None, 4, 4, 512)         2048      \n","_________________________________________________________________\n","Pool4 (MaxPooling2D)         (None, 2, 2, 512)         0         \n","_________________________________________________________________\n","Conv11 (Conv2D)              (None, 2, 2, 512)         2359808   \n","_________________________________________________________________\n","Conv12 (Conv2D)              (None, 2, 2, 512)         2359808   \n","_________________________________________________________________\n","Conv13 (Conv2D)              (None, 2, 2, 512)         2359808   \n","_________________________________________________________________\n","Norm5 (BatchNormalization)   (None, 2, 2, 512)         2048      \n","_________________________________________________________________\n","Pool5 (MaxPooling2D)         (None, 1, 1, 512)         0         \n","_________________________________________________________________\n","Flat (Flatten)               (None, 512)               0         \n","_________________________________________________________________\n","Dense1 (Dense)               (None, 512)               262656    \n","_________________________________________________________________\n","Norm6 (BatchNormalization)   (None, 512)               2048      \n","_________________________________________________________________\n","Dense2 (Dense)               (None, 512)               262656    \n","_________________________________________________________________\n","Norm7 (BatchNormalization)   (None, 512)               2048      \n","_________________________________________________________________\n","Dense3 (Dense)               (None, 10)                5130      \n","=================================================================\n","Total params: 15,255,114\n","Trainable params: 15,250,122\n","Non-trainable params: 4,992\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lJiRjm3Anwdl"},"source":["<br><font size=\"5\">モデルコンパイル</font>"]},{"cell_type":"code","metadata":{"id":"Ek2HoDHznwdl"},"source":["from keras.optimizers import Adam\n","\n","adam = Adam(lr=0.001) # 最適化手法(Adam)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=adam, \n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vX5axR_Snwdm"},"source":["<br><font size=\"5\">学習データと訓練データの精度と損失値</font>"]},{"cell_type":"code","metadata":{"id":"QXnLfRmWnwdm","outputId":"87f9a8fe-e0b1-44fc-9000-9781545fcaeb"},"source":["score = model.evaluate(x_train,y_train, batch_size=128) #データセット\n","print(\"loss = {:.5f}\".format(score[0]))\n","print(\"accuracy = {:.5f}\".format(score[1]))\n","score = model.evaluate(x_test,y_test, batch_size=128) #訓練データ\n","print(\"loss = {:.5f}\".format(score[0]))\n","print(\"accuracy = {:.5f}\".format(score[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["50000/50000 [==============================] - 5s 105us/step\n","loss = 1.59843\n","accuracy = 0.76464\n","10000/10000 [==============================] - 1s 103us/step\n","loss = 2.30055\n","accuracy = 0.61930\n","time:6.5620129108428955[sec]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LMvU6ksbnwdm"},"source":["<br><font size=\"5\">訓練データの精度と損失値</font>"]},{"cell_type":"code","metadata":{"id":"jEWZBfccnwdm"},"source":["score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P9weEB_Jnwdn"},"source":["<br><font size=\"5\">量子化後の認識率の比較</font>"]},{"cell_type":"code","metadata":{"id":"JeUCCJA6nwdn","outputId":"5ad2e7ba-1769-4045-ee03-6cf3a3687e92"},"source":["from PIL import Image\n","import sys\n","\n","\n","pre = model.predict(x_test)\n","count=0\n","for i,v in enumerate(pre):\n","    pre_ans = v.argmax()\n","    ans = y_test[i].argmax()\n","    dat = x_test[i]\n","    if ans == pre_ans:\n","        count=count+1\n","        continue\n","result=count/10000  #訓練画像の枚数\n","\n","data1=['val_acc',result]\n","result=result*100\n","result=np.round(result,1)\n","data2=['result(%)',result]\n","\n","print(result)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lBDEsB0bnwdn"},"source":["<br><font size=\"5\">ベクトル量子化</font>"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"3_EyZd8rnwdn","outputId":"34eaba92-6981-4187-cdf3-8c382eed4666"},"source":["#np.whereでラベルを各クラスタの重心に置き換えて，それを重み係数に代入して保存する．\n","#畳み込み層第1層目から13層目まで各層の重み係数をクラスタリング(kmeans++)してる．\n","\n","import csv\n","from sklearn.cluster import KMeans\n","\n","#filename=\"VGG16_cifar10_epoch200\"\n","\n","#各変数の初期化\n","entropy_sum=0\n","size_sum=0\n","bit_all=0\n","entropy_all=0\n","\n","#各層ごとにコードブック(代表ベクトルを集めたデータ群)を作成して，ベクトル量子化を行う関数\n","def Vector_Quantization(w,b,shape,num,num_2):\n","    \n","    w=w.reshape(int(w.size/num),-1)\n","#     print(\"------------------reshape後----------------------------------------------------\")\n","#     print(w.shape)\n","#     print(w)\n","#     print(\"---------------------------------------------------------------------------------\")\n","    \n","    # K-means++ クラスタリングをおこなう\n","    kmeans_model = KMeans(n_clusters=num_2).fit(w)\n","    # 分類先となったラベルとクラスターを取得する\n","    labels = kmeans_model.labels_\n","    clusters=kmeans_model.cluster_centers_\n","    \n","    #print(labels)\n","    \n","    #ラベルとクラスターをリスト型に変換する\n","    labels_2=labels.tolist()\n","    clusters_2=clusters.tolist()\n","    \n","    #print(labels_2)\n","    \n","    #ここで各コードブックの発生回数を求めている，binsはクラスター数を入れている\n","    histogram,_=np.histogram(labels_2,bins=num_2)\n","    #print(histogram)\n","    #ここで平均情報量を求めている,baseはlogの底を表している\n","    entropy=scipy.stats.entropy(histogram,base=2)\n","    \n","    #エントロピー算出の際，ベクトル量子化だと次元数で割る\n","    vq_entropy=(entropy/num)*w.size\n","    \n","#     print(\"------------------ベクトル量子化後のエントロピー----------------------------------------------------\")\n","#     print(vq_entropy)\n","#     print(\"-------------------------------------------------------------------------------------------------\")\n","    \n","    #ラベルの数を増やすため，insert(挿入)する位置をここで生成している\n","    square=[0 for a in range(num-1)]\n","    for j in range(1,num):\n","        square[j-1]=[i for i in range(0,int(w.size/num*j),j)]\n","    #実際に同じ値のラベルを挿入してreshapeできるようにしている\n","    data=[0 for a in range(num)]\n","    data[0]=labels    \n","    for j in range(1,num):\n","        data[j]=np.insert(data[j-1],square[j-1],labels_2)\n","    #ここでラベル配列(一次元)を二次元配列にreshapeする\n","    box=data[-1].reshape(int(w.size/num),-1)\n","    \n","#     print(\"------------------ラベル配列の様子----------------------------------------------------\")\n","#     print(box)\n","#     print(\"-------------------------------------------------------------------------------------\")\n","    \n","#     print(\"------------------クラスターの重心----------------------------------------------------\")\n","#     print(clusters)\n","#     print(\"-------------------------------------------------------------------------------------\")\n","    \n","    #全ラベル[次元数の数]をcsvファイルに\n","#     with open(filename+\"_box.csv\", \"a\") as f:\n","#         #writer = csv.writer(f, lineterminator='\\n') # 行末は改行\n","#         writer = csv.writer(f) \n","#         #writer.writerow(b)\n","#         for d in box:\n","#             writer.writerow(d)\n","        \n","    #全クラスターをcsvファイルに\n","#     with open(filename+\"_clusters.csv\", \"a\") as f:\n","#         #writer = csv.writer(f, lineterminator='\\n') # 行末は改行\n","#         writer = csv.writer(f)\n","#         #writer.writerow(clusters)\n","#         for c in clusters:\n","#             writer.writerow(c)\n","        \n","    #ここでラベルだけの配列に各クラスターの重心を代入する\n","    data_array = [0 for b in range(num_2)]\n","    data_array[0] =np.where(box==[0]*num,clusters_2[0],box)\n","    for k in range(1,num_2):\n","        data_array[k] = np.where(data_array[k-1]==[k for jj in range(num)],clusters_2[k],data_array[k-1])\n","    #ここでVQ後の重みを代入する\n","    w=data_array[-1]\n","    \n","#          #全クラスターをcsvファイルに\n","#     with open(filename+\"_weights.csv\", \"a\") as f:\n","#         #writer = csv.writer(f, lineterminator='\\n') # 行末は改行\n","#         writer = csv.writer(f)\n","#         #writer.writerow(clusters)\n","#         writer.writerow(w)\n","    \n","    \"\"\"}\n","    array_1=np.where(data_3==[0,0,0],clusters_2[0],data_3)\n","    array_2=np.where(array_1==[1,1,1],clusters_2[1],array_1)\n","    array_3=np.where(array_2==[2,2,2],clusters_2[2],array_2)\n","    array_4=np.where(array_3==[3,3,3],clusters_2[3],array_3)\n","    array_5=np.where(array_4==[4,4,4],clusters_2[4],array_4)\n","    array_6=np.where(array_5==[5,5,5],clusters_2[5],array_5)\n","    array_7=np.where(array_6==[6,6,6],clusters_2[6],array_6)\n","    array_8=np.where(array_7==[7,7,7],clusters_2[7],array_7)\n","    array_9=np.where(array_8==[8,8,8],clusters_2[8],array_8)\n","    \n","    w=array_9\n","    \"\"\"\n","    #ここで元の重みの型にreshapeする\n","    w=w.reshape(shape)\n","    \n","    #ここで重みを保存\n","    model.get_layer(\"Conv\"+str(i)).set_weights([w,b])\n","    \n","#     print(\"------------------ベクトル量子化後の重み----------------------------------------------------\")\n","#     print(w)\n","#     print(\"------------------------------------------------------------------------------------------\")\n","    \n","    #VQ後のエントロピーと重み係数の個数を返り値としている\n","    return vq_entropy,w.size\n","\n","    \n","#畳み込み層(1~13層目)\n","for i in range(1,14):\n","    w=model.get_layer(\"Conv\"+str(i)).get_weights()[0] #重み\n","    b=model.get_layer(\"Conv\"+str(i)).get_weights()[1] #バイアス\n","    shape=w.shape\n","    print(\"Conv\"+str(i))\n","#     print(\"------------------\"+\"Conv\"+str(i)+\"の重み\"+\"----------------------------------------------------\")\n","#     print(shape)\n","#     print(w)\n","#     print(\"------------------------------------------------------------------------------------------------\")\n","    #(重み，バイアス，重みのshape，入力次元数，分けるクラス数)\n","    entropy,size=Vector_Quantization(w,b,shape,3,10)\n","    #エントロピーと重み係数の個数を足している\n","    entropy_sum+=entropy\n","    size_sum+=size\n","#     print(\"------------------\"+\"Conv\"+str(i)+\"までのエントロピー\"+\"----------------------------------------------------\")\n","#     print(entropy_sum / size_sum )\n","#     print(\"--------------------------------------------------------------------------------------------------------------\")\n","    \n","#ここで全体のエントロピー算出\n","print(entropy_sum / size_sum )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Conv1\n","------------------Conv1の重み----------------------------------------------------\n","(3, 3, 3, 64)\n","------------------------------------------------------------------------------------------------\n","Conv2\n","------------------Conv2の重み----------------------------------------------------\n","(3, 3, 64, 64)\n","------------------------------------------------------------------------------------------------\n","Conv3\n","------------------Conv3の重み----------------------------------------------------\n","(3, 3, 64, 128)\n","------------------------------------------------------------------------------------------------\n","Conv4\n","------------------Conv4の重み----------------------------------------------------\n","(3, 3, 128, 128)\n","------------------------------------------------------------------------------------------------\n","Conv5\n","------------------Conv5の重み----------------------------------------------------\n","(3, 3, 128, 256)\n","------------------------------------------------------------------------------------------------\n","Conv6\n","------------------Conv6の重み----------------------------------------------------\n","(3, 3, 256, 256)\n","------------------------------------------------------------------------------------------------\n","Conv7\n","------------------Conv7の重み----------------------------------------------------\n","(3, 3, 256, 256)\n","------------------------------------------------------------------------------------------------\n","Conv8\n","------------------Conv8の重み----------------------------------------------------\n","(3, 3, 256, 512)\n","------------------------------------------------------------------------------------------------\n","Conv9\n","------------------Conv9の重み----------------------------------------------------\n","(3, 3, 512, 512)\n","------------------------------------------------------------------------------------------------\n","Conv10\n","------------------Conv10の重み----------------------------------------------------\n","(3, 3, 512, 512)\n","------------------------------------------------------------------------------------------------\n","Conv11\n","------------------Conv11の重み----------------------------------------------------\n","(3, 3, 512, 512)\n","------------------------------------------------------------------------------------------------\n","Conv12\n","------------------Conv12の重み----------------------------------------------------\n","(3, 3, 512, 512)\n","------------------------------------------------------------------------------------------------\n","Conv13\n","------------------Conv13の重み----------------------------------------------------\n","(3, 3, 512, 512)\n","------------------------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UUGP24Ixnwdo","outputId":"e6e5e11d-795b-4add-b460-9d655277fa69"},"source":["#第1層目の重み係数を取り出す場合\n","i=1\n","weights=model.get_layer(\"Conv\"+str(i)).get_weights()[0]\n","weights"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[[-2.91950273e-32, -1.32626957e-32, -1.65118892e-02, ...,\n","           2.94441293e-32, -3.08294058e-01, -2.72437302e-32],\n","         [-2.89190800e-32, -2.26068091e-32, -2.23156258e-01, ...,\n","          -2.86312455e-32, -3.97022754e-01, -2.78202926e-32],\n","         [ 2.94850306e-32,  2.44034097e-32, -1.66881815e-01, ...,\n","          -3.02844755e-32, -5.85686862e-01, -2.52653468e-32]],\n","\n","        [[-2.58212028e-32, -6.38503111e-33,  4.15137589e-01, ...,\n","          -2.81060317e-32,  2.59104729e-01, -2.75944801e-32],\n","         [-2.71475101e-32, -2.42194183e-32,  3.90448928e-01, ...,\n","          -2.93331185e-32,  4.70458925e-01, -2.95500766e-32],\n","         [-2.74026394e-32, -2.59474744e-32,  8.45992088e-01, ...,\n","           2.85805053e-32,  1.41120091e-01, -2.68767526e-32]],\n","\n","        [[ 3.00161483e-32, -9.76388530e-33, -5.63182771e-01, ...,\n","          -2.74471760e-32,  1.06783256e-01,  2.73184975e-32],\n","         [ 2.83399169e-32, -2.38070153e-32, -1.46186441e-01, ...,\n","          -2.73738045e-32,  3.84009898e-01,  2.58692452e-32],\n","         [ 3.04744207e-32, -2.90133547e-32, -6.19796030e-02, ...,\n","          -2.58423029e-32, -7.84551352e-02,  2.90892416e-32]]],\n","\n","\n","       [[[ 2.67269946e-32, -9.17550483e-33,  4.42008339e-02, ...,\n","           2.81870321e-32, -1.11267798e-01, -2.89177429e-32],\n","         [-2.86818976e-32, -2.35353761e-32, -8.22369456e-02, ...,\n","           2.72271587e-32, -7.16091767e-02, -3.41757438e-32],\n","         [ 2.85593964e-32,  2.78659781e-32, -2.88534671e-01, ...,\n","           2.82043383e-32,  2.57713437e-01,  2.91855528e-32]],\n","\n","        [[ 2.68013829e-32, -4.63473398e-33,  7.01109618e-02, ...,\n","          -2.73629283e-32,  2.23406240e-01, -2.74048934e-32],\n","         [-2.89180250e-32, -2.23881421e-32,  2.81448156e-01, ...,\n","           2.85974736e-32,  1.39792010e-01, -3.01476333e-32],\n","         [-2.89528050e-32, -2.51718509e-32,  1.79770306e-01, ...,\n","          -2.91173183e-32,  1.82893381e-01,  2.98471211e-32]],\n","\n","        [[-2.66863402e-32, -1.22517993e-32, -5.25735736e-01, ...,\n","          -2.71433959e-32, -4.78783734e-02, -2.65493216e-32],\n","         [-2.79050575e-32, -2.36235691e-32, -3.15663815e-01, ...,\n","          -2.77059199e-32, -2.28648230e-01, -2.76007425e-32],\n","         [-2.71472603e-32,  2.56392686e-32,  7.32525736e-02, ...,\n","          -2.66458297e-32, -9.72489864e-02,  2.79317970e-32]]],\n","\n","\n","       [[[-2.80383115e-32, -2.05538067e-32,  1.06944002e-01, ...,\n","          -2.56039185e-32,  2.51454026e-01,  3.35677458e-32],\n","         [ 3.04230780e-32, -2.77703135e-32,  4.32848185e-01, ...,\n","          -2.72410765e-32,  2.91771561e-01, -2.92935808e-32],\n","         [-2.77473619e-32, -2.75027974e-32, -1.59341782e-01, ...,\n","          -3.02281693e-32,  4.31815743e-01, -2.70256819e-32]],\n","\n","        [[ 2.80290104e-32, -1.97951118e-32,  1.62696391e-01, ...,\n","          -2.71924258e-32, -2.58741826e-01, -2.97219721e-32],\n","         [ 2.67904331e-32, -2.53727341e-32, -3.67383242e-01, ...,\n","          -2.59344470e-32, -5.03381312e-01, -2.76889751e-32],\n","         [-2.93708107e-32,  2.65828203e-32, -7.43497610e-01, ...,\n","          -2.63577484e-32,  1.09448522e-01, -2.72007659e-32]],\n","\n","        [[-3.09330398e-32, -2.24957836e-32,  3.30447137e-01, ...,\n","          -2.55897950e-32, -4.21041204e-03, -2.61543526e-32],\n","         [-2.87157342e-32, -2.48471617e-32,  2.11098157e-02, ...,\n","          -2.51872881e-32, -1.45831510e-01, -2.76206848e-32],\n","         [-2.95316243e-32,  2.50753164e-32,  2.51984119e-01, ...,\n","          -2.54232480e-32, -2.22525656e-01, -2.72872323e-32]]]],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":119}]},{"cell_type":"markdown","metadata":{"id":"dIYUCAkKnwdo"},"source":["<br><font size=\"5\">重み係数(畳み込み層1～13層目)の統計的性質をcsvファイルにまとめる</font>"]},{"cell_type":"code","metadata":{"id":"CPB3tSlXnwdo","outputId":"59a86af7-e5fe-47c9-edd4-a289366d2945"},"source":["import csv\n","\n","#csvファイルのファイル名を決める\n","filename=\"VGG16_cifar100_epoch200_he_normal\"\n","\n","# #1層目の重み係数の統計的性質をそのまま出力する場合\n","# weights=model.get_layer(\"conv2d\").get_weights()[0]\n","# weights=weights.reshape(-1)\n","# max=np.amax(weights) #重み係数の最大値\n","# min=np.amin(weights) #重み係数の最小値\n","# avr=np.mean(weights) #重み係数の平均値\n","# dis=np.var(weights) #重み係数の分散\n","# hensa=np.std(weights) #重み係数の分散\n","# kurtosis = scipy.stats.kurtosis(weights)  #重み係数の尖度\n","# skew = scipy.stats.skew(weights)  #重み係数の歪度\n","# count_2=len(weights)  #重み係数の数\n","# print(\"畳み込み層　1層目\")\n","# print(\"最大値: \"+str(max)+\"　最小値: \"+str(min)+\"　平均値: \"+str(avr)+\"　分散: \"+str(dis)+\"　標準偏差: \"+str(hensa)+\"　歪度: \"+str(skew)+\"　尖度: \"+str(kurtosis)+\"　係数の数: \"+str(count_2))\n","\n","#各層の統計的性質をcsvファイルにまとめる関数(ただ記述しているだけ)\n","def weights_text(i,max,min,avr,dis,hensa,skew,kutosis,count_2): \n","    name=['畳み込み層第'+str(i)+'層目',max,min,avr,hensa,skew,kurtosis]\n","    data1=['最大値',max]\n","    data2=['最小値',min]\n","    data3=['平均値',avr]\n","    data4=['分散',dis]\n","    data5=['標準偏差',hensa]\n","    data6=['歪度',skew]\n","    data7=['尖度',kurtosis]\n","    data8=['係数の数',count_2]\n","\n","    #with open(filename+\"weights_hist_step\"+str(step1)+\".csv\", \"a\") as f:\n","    with open(filename+\".csv\", \"a\") as f:\n","        writer = csv.writer(f, lineterminator='\\n') # 行末は改行\n","        writer.writerow(name) \n","        \n","#         writer.writerow(data1)\n","#         writer.writerow(data2)\n","#         writer.writerow(data3)\n","#         writer.writerow(data4)\n","#         writer.writerow(data5)\n","#         writer.writerow(data6)\n","#         writer.writerow(data7)\n","#         writer.writerow(data8)\n","\n","#各層の統計的性質をcsvファイルに出力するための下準備的な関数\n","def Layer(weights): \n","    \n","    #変数の初期化\n","    count=0\n","    max=0\n","    min=0\n","    avr=0\n","    dis=0\n","    bunsan=0\n","    skew=0\n","    kurtosis=0\n","    count_2=0\n","\n","    for w in weights[:]:\n","        count=count+1\n","    max=np.amax(weights) #重み係数の最大値\n","    min=np.amin(weights) #重み係数の最小値\n","    avr=np.mean(weights) #重み係数の平均値\n","    dis=np.var(weights) #重み係数の分散\n","    hensa=np.std(weights) #重み係数の標準偏差\n","    kurtosis = scipy.stats.kurtosis(weights)  #重み係数の尖度\n","    skew = scipy.stats.skew(weights)  #重み係数の歪度\n","    count_2=len(weights)   #重み係数の個数\n","    #print(max,min,avr,hensa)\n","    #print(\"weights_count: \"+str(count))\n","    \n","    #ここで各統計的性質を返り値としている\n","    return max,min,avr,dis,hensa,kurtosis,skew,count_2\n","\n","#畳み込み層(1～13層)\n","for i in range(1,14):\n","    weights=model.get_layer(\"Conv\"+str(i)).get_weights()[0]\n","    weights=weights.reshape(-1)\n","    \n","    max,min,avr,dis,hensa,kurtosis,skew,count_2=Layer(weights)\n","    print(\"畳み込み層　\"+str(i)+\"層目\")\n","    print(\"最大値: \"+str(max)+\"　最小値: \"+str(min)+\"　平均値: \"+str(avr)+\"　分散: \"+str(dis)+\"　標準偏差: \"+str(hensa)+\"　歪度: \"+str(skew)+\"　尖度: \"+str(kurtosis)+\"　係数の数: \"+str(count_2))\n","    weights_text(i,max,min,avr,dis,hensa,skew,kurtosis,count_2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["畳み込み層　1層目\n","最大値: 0.9078934　最小値: -0.87226695　平均値: -0.006415184　分散: 0.05793824　標準偏差: 0.24070364　歪度: 0.08025909960269928　尖度: 1.886154609142391　係数の数: 1728\n","畳み込み層　2層目\n","最大値: 0.74438274　最小値: -1.0613595　平均値: -0.0054214173　分散: 0.006776381　標準偏差: 0.082318775　歪度: -0.47914958000183105　尖度: 9.949713237478058　係数の数: 36864\n","畳み込み層　3層目\n","最大値: 0.6735768　最小値: -0.7685852　平均値: -0.0043728845　分散: 0.0069083595　標準偏差: 0.08311654　歪度: -0.3882627487182617　尖度: 3.878866248751665　係数の数: 73728\n","畳み込み層　4層目\n","最大値: 0.7277174　最小値: -0.55779904　平均値: -0.0087076835　分散: 0.005726605　標準偏差: 0.07567433　歪度: 0.2297566831111908　尖度: 1.253432417289229　係数の数: 147456\n","畳み込み層　5層目\n","最大値: 0.62851304　最小値: -0.4033634　平均値: -0.0096622575　分散: 0.0053816284　標準偏差: 0.07335959　歪度: 0.21293330192565918　尖度: 0.8723012938243002　係数の数: 294912\n","畳み込み層　6層目\n","最大値: 0.54739255　最小値: -0.31805608　平均値: -0.0055891224　分散: 0.0027864592　標準偏差: 0.052786924　歪度: 0.39123547077178955　尖度: 3.5100582465190353　係数の数: 589824\n","畳み込み層　7層目\n","最大値: 0.5718624　最小値: -0.27715802　平均値: -0.0038936755　分散: 0.0019084164　標準偏差: 0.043685425　歪度: 0.48553383350372314　尖度: 5.893505786687783　係数の数: 589824\n","畳み込み層　8層目\n","最大値: 0.47745243　最小値: -0.34737873　平均値: -0.0020293936　分散: 0.0005895454　標準偏差: 0.024280556　歪度: -0.03704242408275604　尖度: 24.26892933836418　係数の数: 1179648\n","畳み込み層　9層目\n","最大値: 0.48517936　最小値: -0.33074805　平均値: -0.0001334729　分散: 7.984368e-05　標準偏差: 0.008935529　歪度: 3.738644599914551　尖度: 283.49017337066016　係数の数: 2359296\n","畳み込み層　10層目\n","最大値: 0.4617613　最小値: -0.28203452　平均値: -8.490283e-05　分散: 4.27585e-05　標準偏差: 0.0065389983　歪度: 7.249682903289795　尖度: 553.6334199932191　係数の数: 2359296\n","畳み込み層　11層目\n","最大値: 0.61068493　最小値: -0.46001637　平均値: -3.7495786e-06　分散: 2.2012311e-05　標準偏差: 0.004691728　歪度: 8.113118171691895　尖度: 1965.7095637986856　係数の数: 2359296\n","畳み込み層　12層目\n","最大値: 1.0542921　最小値: -0.37667924　平均値: 5.7773773e-06　分散: 7.21029e-06　標準偏差: 0.0026851983　歪度: 147.22361755371094　尖度: 45543.234574696886　係数の数: 2359296\n","畳み込み層　13層目\n","最大値: 0.78134066　最小値: -0.3661907　平均値: 5.659792e-06　分散: 8.150738e-06　標準偏差: 0.0028549498　歪度: 85.12088012695312　尖度: 17317.47007964668　係数の数: 2359296\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ENSWT84cnwdp"},"source":["<br><font size=\"5\">重み係数(畳み込み層1～13層目)の分布をそれぞれtifファイルに出力</font>"]},{"cell_type":"code","metadata":{"id":"FpIFpTh_nwdp"},"source":["\n","#ファイル名を決める\n","f_model = './VGG16_cifar100_epoch200_he_normal/weight_1.0_bins100'\n","\n","def weights_hist(weights_array,i): \n","    fig = plt.figure()\n","    ax = fig.add_subplot(1, 1, 1)\n","    #x軸(重み係数)\n","    ax.set_xlabel('weight',fontsize=12)\n","    #y軸(発生確率)\n","    ax.set_ylabel('probability(%)',fontsize=12)\n","    \n","    #ax.set_xticks([-0.08, -0.04, 0, 0.04, 0.08]) \n","    #ax.set_yticks([0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30])\n","    #ax.set_yticks([0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52])\n","    plt.title(\"畳み込み層　\"+str(i)+\"層目\",fontname=\"MS Gothic\")\n","    plt.tick_params(labelsize=12)\n","    #ax.set_xticks([-0.08, -0.04, 0, 0.04, 0.08]) \n","    #ax.set_yticks([0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30])\n","    #ax.set_yticks([0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52])\n","    #plt.tick_params(labelsize=12)\n","    plt.hist(weights_array,range=(-1.0,1.0),bins=100,density=True)\n","    #plt.hist(weights_array, range=(-0.11,0.11), density=True, histtype='barstacked', ec='black')\n","    #ax.hist(weights_array)\n","    plt.savefig(os.path.join(f_model, \"畳み込み層\"+str(i)+\"層目_weight\"+\".tif\")) \n","\n","#畳み込み層(1～13層)\n","for i in range(1,14):\n","    weights=model.get_layer(\"Conv\"+str(i)).get_weights()[0]\n","    weights=weights.reshape(-1)\n","    weights_hist(weights,i)    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aZiATKHvnwdp"},"source":[""],"execution_count":null,"outputs":[]}]}